{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lgnPvgw8DyOq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "id": "iznsv0cPD4N2"
   },
   "outputs": [],
   "source": [
    "# Created a dataset for a Regression Problem  : \n",
    "df = pd.DataFrame([[3.8,8,4],[3.7,9,5],[3.6,10,6],[3.5,12,7]], columns=['cgpa', 'profile_score', 'lpa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "Icg8cssxEJ4U",
    "outputId": "faf04b08-f325-4e7c-eba6-e002fd3fbc9f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cgpa</th>\n",
       "      <th>profile_score</th>\n",
       "      <th>lpa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.7</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.5</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cgpa  profile_score  lpa\n",
       "0   3.8              8    4\n",
       "1   3.7              9    5\n",
       "2   3.6             10    6\n",
       "3   3.5             12    7"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "id": "gOL7f79VEKQn"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "\n",
    "  np.random.seed(3)\n",
    "  parameters = {}\n",
    "  L = len(layer_dims)\n",
    "\n",
    "  for l in range(1, L):\n",
    "\n",
    "    parameters['W' + str(l)] = np.ones((layer_dims[l-1], layer_dims[l]))*0.1 # initialize weights = 0.1\n",
    "    parameters['b' + str(l)] = np.zeros((layer_dims[l], 1)) # initialize biases = 0 \n",
    "\n",
    "\n",
    "  return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VkduRBgNIp-z",
    "outputId": "c0c32750-846b-499c-fc16-9f542ad3cdd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.1, 0.1],\n",
       "        [0.1, 0.1]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[0.1],\n",
       "        [0.1]]),\n",
       " 'b2': array([[0.]])}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass the architecture of the Neural Network \n",
    "initialize_parameters([2,2,1]) \n",
    "# 2 neurons in the first hidden layer , 2 neurons in the second hidden layer\n",
    "# 2 neurons in the first hidden layer , 2 neurons in the third hidden layer\n",
    "# 1 neuron at the output layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### these are the weights between input layer and first hidden layer : \n",
    "### 'W1': array([[0.1, 0.1],\n",
    "###      [0.1, 0.1]])\n",
    "\n",
    "### these are the biases of the first hidden layer : \n",
    "### 'b1': array([[0.],\n",
    "###     [0.]]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "ZghmdgJzEg0D"
   },
   "outputs": [],
   "source": [
    "# linear_forward calculates the output of a neuron : \n",
    "# output of a neuron depends on 3 things : prev_output , weights of that neuron , bias of that neuron \n",
    "def linear_forward(A_prev, W, b):\n",
    "\n",
    "  Z = np.dot(W.T, A_prev) + b # Compute the linear part of forward propagation: Z = W^T * A_prev + b\n",
    "\n",
    "  return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "5n5DFL99Gacw"
   },
   "outputs": [],
   "source": [
    "# Forward Propagation through all layers:\n",
    "def L_layer_forward(X, parameters):\n",
    "\n",
    "    A = X  # Initialize input data as the first activation\n",
    "    L = len(parameters) // 2  # Number of layers in the neural network\n",
    "\n",
    "    for l in range(1, L + 1):  # Loop through each layer\n",
    "        A_prev = A  # Store the activation of the previous layer\n",
    "        Wl = parameters['W' + str(l)]  # Weights for the current layer\n",
    "        bl = parameters['b' + str(l)]  # Biases for the current layer\n",
    "\n",
    "        # Print current layer details (useful for debugging)\n",
    "        print(\"A\" + str(l - 1) + \": \", A_prev)  # Previous activation\n",
    "        print(\"W\" + str(l) + \": \", Wl)  # Current layer weights\n",
    "        print(\"b\" + str(l) + \": \", bl)  # Current layer biases\n",
    "        print(\"--\" * 20)  # Separator for readability\n",
    "\n",
    "        # Compute activation for the current layer using linear forward propagation\n",
    "        A = linear_forward(A_prev, Wl, bl)\n",
    "        print(\"A\" + str(l) + \": \", A)  # Print the activation of the current layer\n",
    "        print(\"**\" * 20)  # Separator for readability\n",
    "\n",
    "    return A, A_prev  # Return the final activation and the previous activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cgpa</th>\n",
       "      <th>profile_score</th>\n",
       "      <th>lpa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.8</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.7</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.5</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cgpa  profile_score  lpa\n",
       "0   3.8              8    4\n",
       "1   3.7              9    5\n",
       "2   3.6             10    6\n",
       "3   3.5             12    7"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ijULWqvJu0O",
    "outputId": "195c3602-d8a5-44a4-d224-278cd274a0ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0:  [[3.8]\n",
      " [8. ]]\n",
      "W1:  [[0.1 0.1]\n",
      " [0.1 0.1]]\n",
      "b1:  [[0.]\n",
      " [0.]]\n",
      "----------------------------------------\n",
      "A1:  [[1.18]\n",
      " [1.18]]\n",
      "****************************************\n",
      "A1:  [[1.18]\n",
      " [1.18]]\n",
      "W2:  [[0.1]\n",
      " [0.1]]\n",
      "b2:  [[0.]]\n",
      "----------------------------------------\n",
      "A2:  [[0.236]]\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[0].reshape(2, 1)  # Extract 'Cgpa' and 'Interview-score' as input features and reshape to (2, 1)\n",
    "y = df[['lpa']].values[0][0]  # Extract the target salary value from the dataset\n",
    "\n",
    "# Initialize parameters for a neural network with 2 input neurons, 2 hidden layer neurons, and 1 output neuron\n",
    "parameters = initialize_parameters([2, 2, 1])\n",
    "\n",
    "# Perform forward propagation through the neural network\n",
    "y_hat, A1 = L_layer_forward(X, parameters)  # y_hat is the predicted output, A1 is the activation from the hidden layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "id": "ZrhNecsGML3E"
   },
   "outputs": [],
   "source": [
    "y_hat = y_hat[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23600000000000004"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted output for the first row / student : \n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.167695999999998"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss for the fist row : \n",
    "\n",
    "loss = (y - y_hat)**2\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWPvGGQcJ-Wq",
    "outputId": "2700de8a-b50d-4183-f277-0b21a5210def"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.18],\n",
       "       [1.18]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# activation / outputs of the first layer : \n",
    "A1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step By Step How the values of Weights and Biases are Optimizing By Backpropagation : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.1, 0.1],\n",
       "        [0.1, 0.1]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[0.1],\n",
       "        [0.1]]),\n",
       " 'b2': array([[0.]])}"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "id": "EQWgVKAyNIr2"
   },
   "outputs": [],
   "source": [
    "update_parameters(parameters,y,y_hat,A1,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sTSNat7KNTm7",
    "outputId": "f3d3d349-c993-4d2f-a720-48e0e8433db9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.10311475, 0.10655737],\n",
       "        [0.10311475, 0.10655737]]),\n",
       " 'b1': array([[0.00081967],\n",
       "        [0.00081967]]),\n",
       " 'W2': array([[0.10888304],\n",
       "        [0.10888304]]),\n",
       " 'b2': array([[0.007528]])}"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bT36dOGNNijU",
    "outputId": "050e03e6-b792-474e-b4ad-a41c07ab6687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0:  [[ 3.5]\n",
      " [12. ]]\n",
      "W1:  [[0.10311475 0.10655737]\n",
      " [0.10311475 0.10655737]]\n",
      "b1:  [[0.00081967]\n",
      " [0.00081967]]\n",
      "----------------------------------------\n",
      "A1:  [[1.59909832]\n",
      " [1.65245894]]\n",
      "****************************************\n",
      "A1:  [[1.59909832]\n",
      " [1.65245894]]\n",
      "W2:  [[0.10888304]\n",
      " [0.10888304]]\n",
      "b2:  [[0.007528]]\n",
      "----------------------------------------\n",
      "A2:  [[0.36156744]]\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[3].reshape(2, 1)  # Select the 4th row's 'cgpa' and 'profile_score' as features and reshape to (2, 1)\n",
    "y = df[['lpa']].values[3][0]  # Extract the corresponding target value ('lpa') for the 4th row\n",
    "\n",
    "# Perform forward propagation through the network\n",
    "y_hat, A1 = L_layer_forward(X, parameters)  # y_hat is the predicted output, A1 is the activation from the hidden layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "id": "kXN473kWNq2F"
   },
   "outputs": [],
   "source": [
    "update_parameters(parameters,y,y_hat,A1,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hjKso9pQNti8",
    "outputId": "56ff54b1-5b11-44d2-974e-849989291bad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.10916103, 0.12728745],\n",
       "        [0.10919395, 0.12740033]]),\n",
       " 'b1': array([[0.00254718],\n",
       "        [0.00255658]]),\n",
       " 'W2': array([[0.13011405],\n",
       "        [0.13082251]]),\n",
       " 'b2': array([[0.02080487]])}"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "id": "zyWoiZWYL8ft"
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, y, y_hat, A1, X):\n",
    "   \n",
    "    # Update weights and bias for the second (output) layer\n",
    "    parameters['W2'][0][0] += 0.001 * 2 * (y - y_hat) * A1[0][0].item()  # Gradient adjustment for W2[0][0]\n",
    "    parameters['W2'][1][0] += 0.001 * 2 * (y - y_hat) * A1[1][0].item()  # Gradient adjustment for W2[1][0]\n",
    "    parameters['b2'][0][0] += 0.001 * 2 * (y - y_hat)                   # Update bias for the second layer\n",
    "\n",
    "    # Update weights and bias for the first (hidden) layer\n",
    "    parameters['W1'][0][0] += 0.001 * 2 * (y - y_hat) * parameters['W2'][0][0].item() * X[0][0].item()  # W1[0][0]\n",
    "    parameters['W1'][0][1] += 0.001 * 2 * (y - y_hat) * parameters['W2'][0][0].item() * X[1][0].item()  # W1[0][1]\n",
    "    parameters['b1'][0][0] += 0.001 * 2 * (y - y_hat) * parameters['W2'][0][0].item()                   # Bias update for first neuron in layer 1\n",
    "\n",
    "    parameters['W1'][1][0] += 0.001 * 2 * (y - y_hat) * parameters['W2'][1][0].item() * X[0][0].item()  # W1[1][0]\n",
    "    parameters['W1'][1][1] += 0.001 * 2 * (y - y_hat) * parameters['W2'][1][0].item() * X[1][0].item()  # W1[1][1]\n",
    "    parameters['b1'][1][0] += 0.001 * 2 * (y - y_hat) * parameters['W2'][1][0].item()                   # Bias update for second neuron in layer 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W_new = W_old - learning rate * GD(L/W_old)\n",
    "## B_new = B_old - learning rate * GD(L/b_old)\n",
    "\n",
    "\n",
    "Explanation:\n",
    "\n",
    "    Inputs:\n",
    "        parameters: Dictionary containing weights (W1, W2) and biases (b1, b2) for the network.\n",
    "        y: True value (label).\n",
    "        y_hat: Predicted output.\n",
    "        A1: Activation values of the hidden layer.\n",
    "        X: Input features.\n",
    "\n",
    "    Process:\n",
    "        Gradients are computed manually for weight and bias updates.\n",
    "        Learning rate is 0.001.\n",
    "        Adjustments are made based on the gradient of the loss function with respect to each parameter.\n",
    "\n",
    "    Updates:\n",
    "        Second layer (W2, b2): Updated based on the output error (y - y_hat) and hidden layer activations (A1).\n",
    "        First layer (W1, b1): Updated based on the propagated error from the second layer and the input features (X).\n",
    "\n",
    "    Purpose:\n",
    "        This implements a simple manual gradient descent to update the network's parameters and reduce the prediction error in subsequent iterations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y-gTVDlsYoFm",
    "outputId": "f594df70-332f-4144-e903-870185a4a1a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0:  [[3.8]\n",
      " [8. ]]\n",
      "W1:  [[0.1 0.1]\n",
      " [0.1 0.1]]\n",
      "b1:  [[0.]\n",
      " [0.]]\n",
      "----------------------------------------\n",
      "A1:  [[1.18]\n",
      " [1.18]]\n",
      "****************************************\n",
      "A1:  [[1.18]\n",
      " [1.18]]\n",
      "W2:  [[0.1]\n",
      " [0.1]]\n",
      "b2:  [[0.]]\n",
      "----------------------------------------\n",
      "A2:  [[0.236]]\n",
      "****************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.10311475, 0.10655737],\n",
       "        [0.10311475, 0.10655737]]),\n",
       " 'b1': array([[0.00081967],\n",
       "        [0.00081967]]),\n",
       " 'W2': array([[0.10888304],\n",
       "        [0.10888304]]),\n",
       " 'b2': array([[0.007528]])}"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[0].reshape(2,1)  # Extract first row's 'cgpa' and 'profile_score', reshape to (2,1)\n",
    "y = df[['lpa']].values[0][0]  # Extract the target value ('lpa') for the first row\n",
    "\n",
    "# Initialize parameters for a neural network with 2 input neurons, 2 hidden layer neurons, and 1 output neuron\n",
    "parameters = initialize_parameters([2, 2, 1])\n",
    "\n",
    "# Perform forward propagation\n",
    "y_hat, A1 = L_layer_forward(X, parameters)  # y_hat: predicted output, A1: hidden layer activations\n",
    "y_hat = y_hat[0][0]  # Convert the predicted output from a 1x1 matrix to a scalar\n",
    "\n",
    "# Update the parameters using the custom update function\n",
    "update_parameters(parameters, y, y_hat, A1, X)\n",
    "\n",
    "# Print the updated parameters\n",
    "parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgrNh8YGexbz",
    "outputId": "8518ae78-89b3-49aa-9f31-efe95ac3e949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0:  [[3.7]\n",
      " [9. ]]\n",
      "W1:  [[0.10311475 0.10655737]\n",
      " [0.10311475 0.10655737]]\n",
      "b1:  [[0.00081967]\n",
      " [0.00081967]]\n",
      "----------------------------------------\n",
      "A1:  [[1.31037702]\n",
      " [1.3540983 ]]\n",
      "****************************************\n",
      "A1:  [[1.31037702]\n",
      " [1.3540983 ]]\n",
      "W2:  [[0.10888304]\n",
      " [0.10888304]]\n",
      "b2:  [[0.007528]]\n",
      "----------------------------------------\n",
      "A2:  [[0.29764417]]\n",
      "****************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.10733244, 0.1168166 ],\n",
       "        [0.10734674, 0.11685141]]),\n",
       " 'b1': array([[0.00195959],\n",
       "        [0.00196345]]),\n",
       " 'W2': array([[0.12120676],\n",
       "        [0.12161794]]),\n",
       " 'b2': array([[0.01693271]])}"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[1].reshape(2,1)  # Select 2nd row's 'cgpa' and 'profile_score' as input features, reshape to (2x1)\n",
    "y = df[['lpa']].values[1][0]  # Extract target output ('lpa') for the 2nd row\n",
    "\n",
    "# Perform forward propagation\n",
    "y_hat, A1 = L_layer_forward(X, parameters)  # Forward pass: Get predicted output and hidden layer activations\n",
    "y_hat = y_hat[0][0]  # Convert predicted output from 1x1 matrix to scalar\n",
    "\n",
    "# Update parameters using gradient descent\n",
    "update_parameters(parameters, y, y_hat, A1, X)\n",
    "\n",
    "# Print updated parameters\n",
    "parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7QAO9dunfCPn",
    "outputId": "8b5f3eb6-7d5e-4454-86ac-67e7d91ed899"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0:  [[ 3.6]\n",
      " [10. ]]\n",
      "W1:  [[0.10733244 0.1168166 ]\n",
      " [0.10734674 0.11685141]]\n",
      "b1:  [[0.00195959]\n",
      " [0.00196345]]\n",
      "----------------------------------------\n",
      "A1:  [[1.4618238]\n",
      " [1.5910173]]\n",
      "****************************************\n",
      "A1:  [[1.4618238]\n",
      " [1.5910173]]\n",
      "W2:  [[0.12120676]\n",
      " [0.12161794]]\n",
      "b2:  [[0.01693271]]\n",
      "----------------------------------------\n",
      "A2:  [[0.38761189]]\n",
      "****************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.11289336, 0.13226362],\n",
       "        [0.11298289, 0.13250736]]),\n",
       " 'b1': array([[0.00350429],\n",
       "        [0.00352905]]),\n",
       " 'W2': array([[0.1376154 ],\n",
       "        [0.13947676]]),\n",
       " 'b2': array([[0.02815749]])}"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[2].reshape(2,1) # Shape(no of features, no. of training exaplme)\n",
    "y = df[['lpa']].values[2][0]\n",
    "\n",
    "y_hat,A1 = L_layer_forward(X,parameters)\n",
    "y_hat = y_hat[0][0]\n",
    "\n",
    "update_parameters(parameters,y,y_hat,A1,X)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vog84hecfGI7",
    "outputId": "09e6fcb3-2534-468f-e5f5-89e4743097d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0:  [[ 3.5]\n",
      " [12. ]]\n",
      "W1:  [[0.11289336 0.13226362]\n",
      " [0.11298289 0.13250736]]\n",
      "b1:  [[0.00350429]\n",
      " [0.00352905]]\n",
      "----------------------------------------\n",
      "A1:  [[1.75442571]\n",
      " [2.05654007]]\n",
      "****************************************\n",
      "A1:  [[1.75442571]\n",
      " [2.05654007]]\n",
      "W2:  [[0.1376154 ]\n",
      " [0.13947676]]\n",
      "b2:  [[0.02815749]]\n",
      "----------------------------------------\n",
      "A2:  [[0.55643303]]\n",
      "****************************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.1201203 , 0.15704171],\n",
       "        [0.1204694 , 0.15817539]]),\n",
       " 'b1': array([[0.00556913],\n",
       "        [0.00566805]]),\n",
       " 'W2': array([[0.16022492],\n",
       "        [0.16597966]]),\n",
       " 'b2': array([[0.04104462]])}"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df[['cgpa', 'profile_score']].values[3].reshape(2,1) # Shape(no of features, no. of training exaplme)\n",
    "y = df[['lpa']].values[3][0]\n",
    "\n",
    "y_hat,A1 = L_layer_forward(X,parameters)\n",
    "y_hat = y_hat[0][0]\n",
    "\n",
    "update_parameters(parameters,y,y_hat,A1,X)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S-ZcZmJafwkc",
    "outputId": "8af42b77-b968-4b76-e598-8e6006a56219"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0:  [[3.8]\n",
      " [8. ]]\n",
      "W1:  [[0.1 0.1]\n",
      " [0.1 0.1]]\n",
      "b1:  [[0.]\n",
      " [0.]]\n",
      "----------------------------------------\n",
      "A1:  [[1.18]\n",
      " [1.18]]\n",
      "****************************************\n",
      "A1:  [[1.18]\n",
      " [1.18]]\n",
      "W2:  [[0.1]\n",
      " [0.1]]\n",
      "b2:  [[0.]]\n",
      "----------------------------------------\n",
      "A2:  [[0.236]]\n",
      "****************************************\n",
      "A0:  [[3.7]\n",
      " [9. ]]\n",
      "W1:  [[0.10311475 0.10655737]\n",
      " [0.10311475 0.10655737]]\n",
      "b1:  [[0.00081967]\n",
      " [0.00081967]]\n",
      "----------------------------------------\n",
      "A1:  [[1.31037702]\n",
      " [1.3540983 ]]\n",
      "****************************************\n",
      "A1:  [[1.31037702]\n",
      " [1.3540983 ]]\n",
      "W2:  [[0.10888304]\n",
      " [0.10888304]]\n",
      "b2:  [[0.007528]]\n",
      "----------------------------------------\n",
      "A2:  [[0.29764417]]\n",
      "****************************************\n",
      "A0:  [[ 3.6]\n",
      " [10. ]]\n",
      "W1:  [[0.10733244 0.1168166 ]\n",
      " [0.10734674 0.11685141]]\n",
      "b1:  [[0.00195959]\n",
      " [0.00196345]]\n",
      "----------------------------------------\n",
      "A1:  [[1.4618238]\n",
      " [1.5910173]]\n",
      "****************************************\n",
      "A1:  [[1.4618238]\n",
      " [1.5910173]]\n",
      "W2:  [[0.12120676]\n",
      " [0.12161794]]\n",
      "b2:  [[0.01693271]]\n",
      "----------------------------------------\n",
      "A2:  [[0.38761189]]\n",
      "****************************************\n",
      "A0:  [[ 3.5]\n",
      " [12. ]]\n",
      "W1:  [[0.11289336 0.13226362]\n",
      " [0.11298289 0.13250736]]\n",
      "b1:  [[0.00350429]\n",
      " [0.00352905]]\n",
      "----------------------------------------\n",
      "A1:  [[1.75442571]\n",
      " [2.05654007]]\n",
      "****************************************\n",
      "A1:  [[1.75442571]\n",
      " [2.05654007]]\n",
      "W2:  [[0.1376154 ]\n",
      " [0.13947676]]\n",
      "b2:  [[0.02815749]]\n",
      "----------------------------------------\n",
      "A2:  [[0.55643303]]\n",
      "****************************************\n",
      "Epoch -  1 ------------------------------------------LOSS ->  27.324575488790423\n",
      "A0:  [[3.8]\n",
      " [8. ]]\n",
      "W1:  [[0.1201203  0.15704171]\n",
      " [0.1204694  0.15817539]]\n",
      "b1:  [[0.00556913]\n",
      " [0.00566805]]\n",
      "----------------------------------------\n",
      "A1:  [[1.42578144]\n",
      " [1.86782963]]\n",
      "****************************************\n",
      "A1:  [[1.42578144]\n",
      " [1.86782963]]\n",
      "W2:  [[0.16022492]\n",
      " [0.16597966]]\n",
      "b2:  [[0.04104462]]\n",
      "----------------------------------------\n",
      "A2:  [[0.57951208]]\n",
      "****************************************\n",
      "A0:  [[3.7]\n",
      " [9. ]]\n",
      "W1:  [[0.12453902 0.16634426]\n",
      " [0.12511632 0.16795839]]\n",
      "b1:  [[0.00673195]\n",
      " [0.00689093]]\n",
      "----------------------------------------\n",
      "A1:  [[1.59357321]\n",
      " [2.13399022]]\n",
      "****************************************\n",
      "A1:  [[1.59357321]\n",
      " [2.13399022]]\n",
      "W2:  [[0.16997866]\n",
      " [0.17875744]]\n",
      "b2:  [[0.0478856]]\n",
      "----------------------------------------\n",
      "A2:  [[0.70022567]]\n",
      "****************************************\n",
      "A0:  [[ 3.6]\n",
      " [10. ]]\n",
      "W1:  [[0.13038349 0.18056056]\n",
      " [0.13138799 0.18321381]]\n",
      "b1:  [[0.00831154]\n",
      " [0.00858597]]\n",
      "----------------------------------------\n",
      "A1:  [[1.79157206]\n",
      " [2.49074208]]\n",
      "****************************************\n",
      "A1:  [[1.79157206]\n",
      " [2.49074208]]\n",
      "W2:  [[0.18368267]\n",
      " [0.19710879]]\n",
      "b2:  [[0.05648515]]\n",
      "----------------------------------------\n",
      "A2:  [[0.87651305]]\n",
      "****************************************\n",
      "A0:  [[ 3.5]\n",
      " [12. ]]\n",
      "W1:  [[0.1378366  0.20126363]\n",
      " [0.13960067 0.20602679]]\n",
      "b1:  [[0.01038184]\n",
      " [0.01086727]]\n",
      "----------------------------------------\n",
      "A1:  [[2.16801794]\n",
      " [3.18761143]]\n",
      "****************************************\n",
      "A1:  [[2.16801794]\n",
      " [3.18761143]]\n",
      "W2:  [[0.20204086]\n",
      " [0.22263136]]\n",
      "b2:  [[0.06673212]]\n",
      "----------------------------------------\n",
      "A2:  [[1.21442261]]\n",
      "****************************************\n",
      "Epoch -  2 ------------------------------------------LOSS ->  22.47770528856957\n",
      "A0:  [[3.8]\n",
      " [8. ]]\n",
      "W1:  [[0.14703504 0.23280114]\n",
      " [0.1501108  0.24206155]]\n",
      "b1:  [[0.01300997]\n",
      " [0.01387017]]\n",
      "----------------------------------------\n",
      "A1:  [[1.77262955]\n",
      " [2.83500685]]\n",
      "****************************************\n",
      "A1:  [[1.77262955]\n",
      " [2.83500685]]\n",
      "W2:  [[0.22712733]\n",
      " [0.25951571]]\n",
      "b2:  [[0.07830328]]\n",
      "----------------------------------------\n",
      "A2:  [[1.21664471]]\n",
      "****************************************\n",
      "A0:  [[3.7]\n",
      " [9. ]]\n",
      "W1:  [[0.15204832 0.2433554 ]\n",
      " [0.15593431 0.25432155]]\n",
      "b1:  [[0.01432925]\n",
      " [0.01540267]]\n",
      "----------------------------------------\n",
      "A1:  [[1.98031678]\n",
      " [3.20471161]]\n",
      "****************************************\n",
      "A1:  [[1.98031678]\n",
      " [3.20471161]]\n",
      "W2:  [[0.23699505]\n",
      " [0.27529737]]\n",
      "b2:  [[0.08386999]]\n",
      "----------------------------------------\n",
      "A2:  [[1.43544394]]\n",
      "****************************************\n",
      "A0:  [[ 3.6]\n",
      " [10. ]]\n",
      "W1:  [[0.1586721  0.25946731]\n",
      " [0.16379867 0.27345108]]\n",
      "b1:  [[0.01611946]\n",
      " [0.01752817]]\n",
      "----------------------------------------\n",
      "A1:  [[2.2253257 ]\n",
      " [3.68612126]]\n",
      "****************************************\n",
      "A1:  [[2.2253257 ]\n",
      " [3.68612126]]\n",
      "W2:  [[0.25111295]\n",
      " [0.29814412]]\n",
      "b2:  [[0.0909991]]\n",
      "----------------------------------------\n",
      "A2:  [[1.74880258]]\n",
      "****************************************\n",
      "A0:  [[ 3.5]\n",
      " [12. ]]\n",
      "W1:  [[0.16693746 0.28242663]\n",
      " [0.17388375 0.30146519]]\n",
      "b1:  [[0.0184154 ]\n",
      " [0.02032958]]\n",
      "----------------------------------------\n",
      "A1:  [[2.68930147]\n",
      " [4.62640508]]\n",
      "****************************************\n",
      "A1:  [[2.68930147]\n",
      " [4.62640508]]\n",
      "W2:  [[0.27003355]\n",
      " [0.32948498]]\n",
      "b2:  [[0.09950149]]\n",
      "----------------------------------------\n",
      "A2:  [[2.35003408]]\n",
      "****************************************\n",
      "Epoch -  3 ------------------------------------------LOSS ->  15.036997292435053\n",
      "A0:  [[3.8]\n",
      " [8. ]]\n",
      "W1:  [[0.17654106 0.31535328]\n",
      " [0.18600887 0.34303703]]\n",
      "b1:  [[0.02115928]\n",
      " [0.0237939 ]]\n",
      "----------------------------------------\n",
      "A1:  [[2.18008626]\n",
      " [3.9664326 ]]\n",
      "****************************************\n",
      "A1:  [[2.18008626]\n",
      " [3.9664326 ]]\n",
      "W2:  [[0.29504387]\n",
      " [0.37251023]]\n",
      "b2:  [[0.10880142]]\n",
      "----------------------------------------\n",
      "A2:  [[2.22955923]]\n",
      "****************************************\n",
      "A0:  [[3.7]\n",
      " [9. ]]\n",
      "W1:  [[0.18061485 0.32392968]\n",
      " [0.1912101  0.35398699]]\n",
      "b1:  [[0.02223133]\n",
      " [0.02516265]]\n",
      "----------------------------------------\n",
      "A1:  [[2.41139716]\n",
      " [4.40958535]]\n",
      "****************************************\n",
      "A1:  [[2.41139716]\n",
      " [4.40958535]]\n",
      "W2:  [[0.30276329]\n",
      " [0.3865549 ]]\n",
      "b2:  [[0.11234231]]\n",
      "----------------------------------------\n",
      "A2:  [[2.54697167]]\n",
      "****************************************\n",
      "A0:  [[ 3.6]\n",
      " [10. ]]\n",
      "W1:  [[0.18632548 0.33782041]\n",
      " [0.1986197  0.37201036]]\n",
      "b1:  [[0.02377475]\n",
      " [0.02716524]]\n",
      "----------------------------------------\n",
      "A1:  [[2.68074354]\n",
      " [4.96342227]]\n",
      "****************************************\n",
      "A1:  [[2.68074354]\n",
      " [4.96342227]]\n",
      "W2:  [[0.31459374]\n",
      " [0.40818857]]\n",
      "b2:  [[0.11724836]]\n",
      "----------------------------------------\n",
      "A2:  [[2.98660577]]\n",
      "****************************************\n",
      "A0:  [[ 3.5]\n",
      " [12. ]]\n",
      "W1:  [[0.19350158 0.35775401]\n",
      " [0.20812496 0.39841384]]\n",
      "b1:  [[0.02576811]\n",
      " [0.02980559]]\n",
      "----------------------------------------\n",
      "A1:  [[3.20052316]\n",
      " [6.06291072]]\n",
      "****************************************\n",
      "A1:  [[3.20052316]\n",
      " [6.06291072]]\n",
      "W2:  [[0.33075002]\n",
      " [0.43810207]]\n",
      "b2:  [[0.12327515]]\n",
      "----------------------------------------\n",
      "A2:  [[3.83802198]]\n",
      "****************************************\n",
      "Epoch -  4 ------------------------------------------LOSS ->  7.057614579281747\n",
      "A0:  [[3.8]\n",
      " [8. ]]\n",
      "W1:  [[0.20127034 0.38438975]\n",
      " [0.21867049 0.43456994]]\n",
      "b1:  [[0.02798775]\n",
      " [0.0328186 ]]\n",
      "----------------------------------------\n",
      "A1:  [[2.54217896]\n",
      " [4.97005923]]\n",
      "****************************************\n",
      "A1:  [[2.54217896]\n",
      " [4.97005923]]\n",
      "W2:  [[0.35098999]\n",
      " [0.47644365]]\n",
      "b2:  [[0.12959911]]\n",
      "----------------------------------------\n",
      "A2:  [[3.38983163]]\n",
      "****************************************\n",
      "A0:  [[3.7]\n",
      " [9. ]]\n",
      "W1:  [[0.20291236 0.38784665]\n",
      " [0.22090802 0.43928053]]\n",
      "b1:  [[0.02841987]\n",
      " [0.03340742]]\n",
      "----------------------------------------\n",
      "A1:  [[2.76736778]\n",
      " [5.4219648 ]]\n",
      "****************************************\n",
      "A1:  [[2.76736778]\n",
      " [5.4219648 ]]\n",
      "W2:  [[0.3540923]\n",
      " [0.4825088]]\n",
      "b2:  [[0.13081944]]\n",
      "----------------------------------------\n",
      "A2:  [[3.72686878]]\n",
      "****************************************\n",
      "A0:  [[ 3.6]\n",
      " [10. ]]\n",
      "W1:  [[0.20631471 0.39612264]\n",
      " [0.22558388 0.45065425]]\n",
      "b1:  [[0.02933942]\n",
      " [0.03467117]]\n",
      "----------------------------------------\n",
      "A1:  [[3.02791122]\n",
      " [5.9672552 ]]\n",
      "****************************************\n",
      "A1:  [[3.02791122]\n",
      " [5.9672552 ]]\n",
      "W2:  [[0.36113875]\n",
      " [0.49631454]]\n",
      "b2:  [[0.13336571]]\n",
      "----------------------------------------\n",
      "A2:  [[4.1884973]]\n",
      "****************************************\n",
      "A0:  [[ 3.5]\n",
      " [12. ]]\n",
      "W1:  [[0.21116806 0.40960416]\n",
      " [0.2323392  0.46941903]]\n",
      "b1:  [[0.03068757]\n",
      " [0.03654765]]\n",
      "----------------------------------------\n",
      "A1:  [[3.55784621]\n",
      " [7.10319056]]\n",
      "****************************************\n",
      "A1:  [[3.55784621]\n",
      " [7.10319056]]\n",
      "W2:  [[0.37210888]\n",
      " [0.51793394]]\n",
      "b2:  [[0.13698871]]\n",
      "----------------------------------------\n",
      "A2:  [[5.13987837]]\n",
      "****************************************\n",
      "Epoch -  5 ------------------------------------------LOSS ->  2.183690769611048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W1': array([[0.21618558, 0.42680708],\n",
       "        [0.23942723, 0.49372083]]),\n",
       " 'b1': array([[0.03212115],\n",
       "        [0.0385728 ]]),\n",
       " 'W2': array([[0.38534494],\n",
       "        [0.54435954]]),\n",
       " 'b2': array([[0.14070895]])}"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = initialize_parameters([2, 2, 1])  # Initialize parameters for a 2-layer network (2 input, 2 hidden, 1 output)\n",
    "epochs = 5  # Number of epochs for training\n",
    "\n",
    "for i in range(epochs):  # Loop over epochs\n",
    "    Loss = []  # Initialize an empty list to store the loss for the current epoch\n",
    "\n",
    "    for j in range(df.shape[0]):  # Loop over each training example in the dataset\n",
    "        X = df[['cgpa', 'profile_score']].values[j].reshape(2, 1)  # Extract and reshape input features for the j-th row\n",
    "        y = df[['lpa']].values[j][0]  # Extract the target value for the j-th row\n",
    "\n",
    "        # Forward propagation\n",
    "        y_hat, A1 = L_layer_forward(X, parameters)  # Compute the predicted output and hidden layer activations\n",
    "        y_hat = y_hat[0][0]  # Convert the predicted output from a 1x1 matrix to scalar\n",
    "\n",
    "        # Parameter update\n",
    "        update_parameters(parameters, y, y_hat, A1, X)  # Update weights and biases using gradient descent\n",
    "\n",
    "        # Compute loss for the current example and append to Loss list\n",
    "        Loss.append((y - y_hat) ** 2)\n",
    "\n",
    "    # Print the mean loss for the current epoch\n",
    "    print('Epoch - ', i + 1, '------------------------------------------LOSS -> ', np.array(Loss).mean())\n",
    "\n",
    "parameters  # Final updated parameters after training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_J6XCJbU_x5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
